{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.1.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "tf.random.set_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the data directory\n",
    "data_dir = Path(\"D:\\E\\Python\\captcha_images_v2\")\n",
    "#Get list of all the images\n",
    "images = list(data_dir.glob(\"*.png\"))\n",
    "# Store all the characters in a set\n",
    "characters = set()\n",
    "\n",
    "# A list to store the length of each captcha\n",
    "captcha_length = []\n",
    "\n",
    "# Store image-label info\n",
    "dataset = []\n",
    "\n",
    "# Iterate over the dataset and store the\n",
    "# information needed\n",
    "for img_path in images:\n",
    "    # 1. Get the label associated with each image\n",
    "    label = img_path.name.split(\".png\")[0]\n",
    "    # 2. Store the length of this cpatcha\n",
    "    captcha_length.append(len(label))\n",
    "    # 3. Store the image-label pair info\n",
    "    dataset.append((str(img_path), label))\n",
    "    \n",
    "    # 4. Store the characters present\n",
    "    for ch in label:\n",
    "        characters.add(ch)\n",
    "\n",
    "# Sort the characters        \n",
    "characters = sorted(characters)\n",
    "\n",
    "# Convert the dataset info into a dataframe\n",
    "dataset = pd.DataFrame(dataset, columns=[\"img_path\", \"label\"], index=None)\n",
    "\n",
    "# Shuffle the dataset\n",
    "dataset = dataset.sample(frac=1.).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#print(\"Number of unqiue charcaters in the whole dataset: \", len(characters))\n",
    "#print(\"Maximum length of any captcha: \", max(Counter(captcha_length).keys()))\n",
    "#print(\"Characters present: \", characters)\n",
    "##dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images:  (1021, 50, 200)\n",
      "Number of training labels:  (1021,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "#training_data, validation_data = train_test_split(dataset, test_size=0.1, random_state=seed)\n",
    "\n",
    "training_data = dataset\n",
    "#validation_data = validation_data.reset_index(drop=True)\n",
    "\n",
    "#print(\"Number of training samples: \", len(training_data))\n",
    "#print(\"Number of validation samples: \", len(validation_data))\n",
    "\n",
    "\n",
    "\n",
    "# Map text to numeric labels \n",
    "char_to_labels = {char:idx for idx, char in enumerate(characters)}\n",
    "\n",
    "# Map numeric labels to text\n",
    "labels_to_char = {val:key for key, val in char_to_labels.items()}\n",
    "\n",
    "\n",
    "\n",
    "# Sanity check for corrupted images\n",
    "def is_valid_captcha(captcha):\n",
    "    for ch in captcha:\n",
    "        if not ch in characters:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "# Store arrays in memory as it's not a muvh big dataset\n",
    "def generate_arrays(df, resize=True, img_height=50, img_width=200):\n",
    "    \"\"\"Generates image array and labels array from a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: dataframe from which we want to read the data\n",
    "        resize (bool)    : whether to resize images or not\n",
    "        img_weidth (int): width of the resized images\n",
    "        img_height (int): height of the resized images\n",
    "        \n",
    "    Returns:\n",
    "        images (ndarray): grayscale images\n",
    "        labels (ndarray): corresponding encoded labels\n",
    "    \"\"\"\n",
    "    \n",
    "    num_items = len(df)\n",
    "    images = np.zeros((num_items, img_height, img_width), dtype=np.float32)\n",
    "    labels = [0]*num_items\n",
    "    \n",
    "    for i in range(num_items):\n",
    "        img = cv2.imread(df[\"img_path\"][i])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if resize: \n",
    "            img = cv2.resize(img, (img_width, img_height))\n",
    "        \n",
    "        img = (img/255.).astype(np.float32)\n",
    "        label = df[\"label\"][i]\n",
    "        \n",
    "        # Add only if it is a valid captcha\n",
    "        if is_valid_captcha(label):\n",
    "            images[i, :, :] = img\n",
    "            labels[i] = label\n",
    "    \n",
    "    return images, np.array(labels)\n",
    "\n",
    "\n",
    "\n",
    "# Build training data\n",
    "training_data, training_labels = generate_arrays(df=training_data)\n",
    "print(\"Number of training images: \", training_data.shape)\n",
    "print(\"Number of training labels: \", training_labels.shape)\n",
    "\n",
    "\n",
    "# Build validation data\n",
    "#validation_data, validation_labels = generate_arrays(df=validation_data)\n",
    "#print(\"Number of validation images: \", validation_data.shape)\n",
    "#print(\"Number of validation labels: \", validation_labels.shape)\n",
    "row_to_be_added = np.array(np.ones(200)) \n",
    "train_data=np.zeros(shape=(1021,200,200), dtype=np.float32)\n",
    "for j in range(1021):\n",
    "    result=training_data[j]\n",
    "    for i in range(150):\n",
    "# Adding row to numpy array \n",
    "        result = np.vstack ((result, row_to_be_added) ) \n",
    "    train_data[j]=result\n",
    "    \n",
    "#valid_data=np.zeros(shape=(103,200,200), dtype=np.float32)\n",
    "#for j in range(103):\n",
    "  #  result=validation_data[j]\n",
    "   # for i in range(150):\n",
    "# Adding row to numpy array \n",
    "    #    result = np.vstack ((result, row_to_be_added) ) \n",
    "   # valid_data[j]=result#\n",
    "# Function to convert   \n",
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \"<go> \"  \n",
    "    \n",
    "    # traverse in the string   \n",
    "    for ele in s:  \n",
    "        str1 += ele   \n",
    "        str1 += \" \"\n",
    "    # return string   \n",
    "    return str1+ '<stop>'  \n",
    "# Python3 program to Split string into characters \n",
    "def split(word): \n",
    "    return [char for char in word] \n",
    "train_labl_result=[0]*1021\n",
    "#valed_labl_result=[0]*103\n",
    "for i in range(1021):\n",
    "    train_labl_result[i]=listToString(split(training_labels[i]))\n",
    "#for i in range(103):\n",
    " #   valed_labl_result[i]=listToString(split(validation_labels[i]))\n",
    "# Driver code \n",
    "#word = validation_labels[0]\n",
    "#print(split(word)) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1021, 7), dtype=float32, numpy=\n",
       "array([[ 2., 18.,  4., ..., 12.,  8.,  3.],\n",
       "       [ 2., 11.,  4., ..., 13., 13.,  3.],\n",
       "       [ 2.,  7., 12., ..., 12.,  7.,  3.],\n",
       "       ...,\n",
       "       [ 2.,  4.,  7., ..., 19., 10.,  3.],\n",
       "       [ 2., 20., 10., ...,  6., 18.,  3.],\n",
       "       [ 2., 14.,  7., ...,  4., 20.,  3.]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oov_token = '<unk>'\n",
    "document_tokenizer = tf.keras.preprocessing.text.Tokenizer(oov_token=oov_token)\n",
    "document_tokenizer.fit_on_texts(train_labl_result)\n",
    "targets = document_tokenizer.texts_to_sequences(train_labl_result)\n",
    "targets = tf.cast(targets, dtype=tf.float32)\n",
    "targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = 1021\n",
    "BATCH_SIZE = 16\n",
    "dataset = tf.data.Dataset.from_tensor_slices((hg, targets)).shuffle(BUFFER_SIZE).batch(BATCH_SIZE)\n",
    "#print(dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_vocab_size = 400\n",
    "decoder_vocab_size = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-6073cba3a2d0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mh\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1021\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1021\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mh\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m    \u001b[1;31m# for i in range(150):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# Adding row to numpy array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_data' is not defined"
     ]
    }
   ],
   "source": [
    "#for i in range(1021):\n",
    "  #  h=train_data[i].T\n",
    "#h,train_data[0]\n",
    "h=np.zeros(shape=(1021,200,200), dtype=np.float32)\n",
    "for j in range(1021):\n",
    "    h[j]=train_data[j].T\n",
    "   # for i in range(150):\n",
    "# Adding row to numpy array \n",
    "    #result = np.vstack ((result, row_to_be_added) ) \n",
    "   # train_data[j]=result\n",
    "hg=np.concatenate((h, train_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_angles(position, i, d_model):\n",
    "    angle_rates = 1 / np.power(1000, (2 * (i // 2)) / np.float32(d_model))\n",
    "    return position * angle_rates\n",
    "  \n",
    "def positional_encoding(position, d_model):\n",
    "    angle_rads = get_angles(\n",
    "        np.arange(position)[:, np.newaxis],\n",
    "        np.arange(d_model)[np.newaxis, :],\n",
    "        d_model\n",
    "    )\n",
    "\n",
    "    # apply sin to even indices in the array; 2i\n",
    "    angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "\n",
    "    # apply cos to odd indices in the array; 2i+1\n",
    "    angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "\n",
    "    pos_encoding = angle_rads[np.newaxis, ...]\n",
    "\n",
    "    return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "def positional_encoding_2(position, d_model):\n",
    "  angle_rads = get_angles(np.arange(position/2)[:, np.newaxis],\n",
    "                          np.arange(d_model)[np.newaxis, :],\n",
    "                          d_model)\n",
    "  \n",
    "  # apply sin to even indices in the array; 2i\n",
    "  angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "  \n",
    "  # apply cos to odd indices in the array; 2i+1\n",
    "  angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "    \n",
    "  pos_encoding = angle_rads[np.newaxis, ...]\n",
    "  pe2= pos_encoding.T \n",
    "  pe2=np.reshape(pe2,(1,200,200))\n",
    "  pe2=np.concatenate((pe2, pos_encoding), axis=1)\n",
    "  return tf.cast(pe2, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def create_look_ahead_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask\n",
    "\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "\n",
    "    dk = tf.cast(tf.shape(k)[-1], tf.float32)\n",
    "    scaled_attention_logits = matmul_qk / tf.math.sqrt(dk)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_attention_logits += (mask * -1e9)  \n",
    "\n",
    "    attention_weights = tf.nn.softmax(scaled_attention_logits, axis=-1)\n",
    "\n",
    "    output = tf.matmul(attention_weights, v)\n",
    "    return output, attention_weights\n",
    "\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "\n",
    "        assert d_model % self.num_heads == 0\n",
    "\n",
    "        self.depth = d_model // self.num_heads\n",
    "\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "        \n",
    "    def split_heads(self, x, batch_size):\n",
    "        x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "    \n",
    "    def call(self, v, k, q, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "\n",
    "        q = self.wq(q)\n",
    "        k = self.wk(k)\n",
    "        v = self.wv(v)\n",
    "       \n",
    "        \n",
    "       \n",
    "        q = self.split_heads(q, batch_size)\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "      \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(\n",
    "            q, k, v, mask)\n",
    "\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model))\n",
    "        output = self.dense(concat_attention)\n",
    "            \n",
    "        return output, attention_weights \n",
    "\n",
    "def point_wise_feed_forward_network(d_model, dff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(dff, activation='relu'),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])\n",
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, training, mask):\n",
    "        attn_output, _ = self.mha(x, x, x, mask)\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = point_wise_feed_forward_network(d_model, dff)\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        attn1, attn_weights_block1 = self.mha1(x, x, x, look_ahead_mask)\n",
    "        attn1 = self.dropout1(attn1, training=training)\n",
    "        out1 = self.layernorm1(attn1 + x)\n",
    "\n",
    "        attn2, attn_weights_block2 = self.mha2(enc_output, enc_output, out1, padding_mask)\n",
    "        attn2 = self.dropout2(attn2, training=training)\n",
    "        out2 = self.layernorm2(attn2 + out1)\n",
    "\n",
    "        ffn_output = self.ffn(out2)\n",
    "        ffn_output = self.dropout3(ffn_output, training=training)\n",
    "        out3 = self.layernorm3(ffn_output + out2)\n",
    "\n",
    "        return out3, attn_weights_block1, attn_weights_block2\n",
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.InputLayer(input_shape=(input_vocab_size,d_model))\n",
    "        #self.dense=tf.keras.layers.Dense(d_model)\n",
    "        self.pos_encoding = positional_encoding_2(maximum_position_encoding, self.d_model)\n",
    "\n",
    "        self.enc_layers = [EncoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "        \n",
    "    def call(self, x, training, mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "       \n",
    "        x = self.embedding(x)\n",
    "       # x=self.dense(x)\n",
    "        \n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        \n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "      \n",
    "\n",
    "       \n",
    "        \n",
    "        #f=self.enc_layers[1](f, training, mask)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.enc_layers[i](x, training, mask)\n",
    "    \n",
    "        return x\n",
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, target_vocab_size, maximum_position_encoding, rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(maximum_position_encoding, d_model)\n",
    "\n",
    "        self.dec_layers = [DecoderLayer(d_model, num_heads, dff, rate) for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(rate)\n",
    "    \n",
    "    def call(self, x, enc_output, training, look_ahead_mask, padding_mask):\n",
    "        seq_len = tf.shape(x)[1]\n",
    "        attention_weights = {}\n",
    "\n",
    "        x = self.embedding(x)\n",
    "        x *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        x += self.pos_encoding[:, :seq_len, :]\n",
    "\n",
    "        x = self.dropout(x, training=training)\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x, block1, block2 = self.dec_layers[i](x, enc_output, training, look_ahead_mask, padding_mask)\n",
    "\n",
    "            attention_weights['decoder_layer{}_block1'.format(i+1)] = block1\n",
    "            attention_weights['decoder_layer{}_block2'.format(i+1)] = block2\n",
    "    \n",
    "        return x, attention_weights\n",
    "#Finally, the Transformer\n",
    "\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, num_layers, d_model, num_heads, dff, input_vocab_size, target_vocab_size, pe_input, pe_target, rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "\n",
    "        self.encoder = Encoder(num_layers, d_model, num_heads, dff, input_vocab_size, pe_input, rate)\n",
    "\n",
    "        self.decoder = Decoder(num_layers, d_model, num_heads, dff, target_vocab_size, pe_target, rate)\n",
    "\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    \n",
    "    def call(self, inp, tar, training, enc_padding_mask, look_ahead_mask, dec_padding_mask):\n",
    "        enc_output = self.encoder(inp, training, enc_padding_mask)\n",
    "\n",
    "        dec_output, attention_weights = self.decoder(tar, enc_output, training, look_ahead_mask, dec_padding_mask)\n",
    "\n",
    "        final_output = self.final_layer(dec_output)\n",
    "\n",
    "        return final_output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyper-params\n",
    "num_layers = 3\n",
    "d_model = 200\n",
    "dff = 400\n",
    "num_heads = 10\n",
    "EPOCHS = 5\n",
    "###########################\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.d_model = tf.cast(self.d_model, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "#######################\n",
    "learning_rate = CustomSchedule(d_model)\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)\n",
    "####################################################\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "#########################\n",
    "transformer66 = Transformer(\n",
    "    num_layers, \n",
    "    d_model, \n",
    "    num_heads, \n",
    "    dff,\n",
    "    encoder_vocab_size, \n",
    "    25, \n",
    "    pe_input=encoder_vocab_size, \n",
    "    pe_target=7,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latest checkpoint restored!!\n"
     ]
    }
   ],
   "source": [
    "####################mask\n",
    "def create_masks(inp, tar):\n",
    "    enc_padding_mask = create_padding_mask(inp)\n",
    "    dec_padding_mask = create_padding_mask(inp)\n",
    "\n",
    "    look_ahead_mask = create_look_ahead_mask(tf.shape(tar)[1])\n",
    "    dec_target_padding_mask = create_padding_mask(tar)\n",
    "    combined_mask = tf.maximum(dec_target_padding_mask, look_ahead_mask)\n",
    "  \n",
    "    return enc_padding_mask, combined_mask, dec_padding_mask\n",
    "###########################checkpoint\n",
    "checkpoint_path = \"checkpoints\"\n",
    "\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer66, optimizer=optimizer)\n",
    "\n",
    "ckpt_manager  = tf.train.CheckpointManager(\n",
    "    ckpt, directory=\"New folder (3)\", max_to_keep=5)\n",
    "\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print ('Latest checkpoint restored!!')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "#################################training step\n",
    "@tf.function\n",
    "def train_step(inp, tar):\n",
    "    tar_inp = tar[:, :-1]\n",
    "    tar_real = tar[:, 1:]\n",
    "\n",
    "    enc_padding_mask, combined_mask, dec_padding_mask = create_masks(inp, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, _ = transformer66(\n",
    "            inp, tar_inp, \n",
    "            True, \n",
    "            None, \n",
    "            combined_mask, \n",
    "            None\n",
    "        )\n",
    "        loss = loss_function(tar_real, predictions)\n",
    "\n",
    "    gradients = tape.gradient(loss, transformer66.trainable_variables)    \n",
    "    optimizer.apply_gradients(zip(gradients, transformer66.trainable_variables))\n",
    "\n",
    "    train_loss(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 Batch 0 Loss 0.3000\n",
      "Epoch 1 Batch 1 Loss 0.2631\n",
      "Epoch 1 Batch 2 Loss 0.2363\n",
      "Epoch 1 Batch 3 Loss 0.2495\n",
      "Epoch 1 Batch 4 Loss 0.2450\n",
      "Epoch 1 Batch 5 Loss 0.2505\n",
      "Epoch 1 Batch 6 Loss 0.2522\n",
      "Epoch 1 Batch 7 Loss 0.2387\n",
      "Epoch 1 Batch 8 Loss 0.2377\n",
      "Epoch 1 Batch 9 Loss 0.2295\n",
      "Epoch 1 Batch 10 Loss 0.2344\n",
      "Epoch 1 Batch 11 Loss 0.2293\n",
      "Epoch 1 Batch 12 Loss 0.2224\n",
      "Epoch 1 Batch 13 Loss 0.2161\n",
      "Epoch 1 Batch 14 Loss 0.2292\n",
      "Epoch 1 Batch 15 Loss 0.2226\n",
      "Epoch 1 Batch 16 Loss 0.2185\n",
      "Epoch 1 Batch 17 Loss 0.2245\n",
      "Epoch 1 Batch 18 Loss 0.2230\n",
      "Epoch 1 Batch 19 Loss 0.2220\n",
      "Epoch 1 Batch 20 Loss 0.2212\n",
      "Epoch 1 Batch 21 Loss 0.2381\n",
      "Epoch 1 Batch 22 Loss 0.2424\n",
      "Epoch 1 Batch 23 Loss 0.2502\n",
      "Epoch 1 Batch 24 Loss 0.2506\n",
      "Epoch 1 Batch 25 Loss 0.2497\n",
      "Epoch 1 Batch 26 Loss 0.2470\n",
      "Epoch 1 Batch 27 Loss 0.2584\n",
      "Epoch 1 Batch 28 Loss 0.2565\n",
      "Epoch 1 Batch 29 Loss 0.2559\n",
      "Epoch 1 Batch 30 Loss 0.2543\n",
      "Epoch 1 Batch 31 Loss 0.2559\n",
      "Epoch 1 Batch 32 Loss 0.2555\n",
      "Epoch 1 Batch 33 Loss 0.2611\n",
      "Epoch 1 Batch 34 Loss 0.2601\n",
      "Epoch 1 Batch 35 Loss 0.2590\n",
      "Epoch 1 Batch 36 Loss 0.2639\n",
      "Epoch 1 Batch 37 Loss 0.2629\n",
      "Epoch 1 Batch 38 Loss 0.2620\n",
      "Epoch 1 Batch 39 Loss 0.2611\n",
      "Epoch 1 Batch 40 Loss 0.2595\n",
      "Epoch 1 Batch 41 Loss 0.2593\n",
      "Epoch 1 Batch 42 Loss 0.2569\n",
      "Epoch 1 Batch 43 Loss 0.2583\n",
      "Epoch 1 Batch 44 Loss 0.2569\n",
      "Epoch 1 Batch 45 Loss 0.2559\n",
      "Epoch 1 Batch 46 Loss 0.2557\n",
      "Epoch 1 Batch 47 Loss 0.2564\n",
      "Epoch 1 Batch 48 Loss 0.2541\n",
      "Epoch 1 Batch 49 Loss 0.2534\n",
      "Epoch 1 Batch 50 Loss 0.2524\n",
      "Epoch 1 Batch 51 Loss 0.2518\n",
      "Epoch 1 Batch 52 Loss 0.2516\n",
      "Epoch 1 Batch 53 Loss 0.2525\n",
      "Epoch 1 Batch 54 Loss 0.2491\n",
      "Epoch 1 Batch 55 Loss 0.2517\n",
      "Epoch 1 Batch 56 Loss 0.2497\n",
      "Epoch 1 Batch 57 Loss 0.2488\n",
      "Epoch 1 Batch 58 Loss 0.2485\n",
      "Epoch 1 Batch 59 Loss 0.2483\n",
      "Epoch 1 Batch 60 Loss 0.2492\n",
      "Epoch 1 Batch 61 Loss 0.2485\n",
      "Epoch 1 Batch 62 Loss 0.2479\n",
      "Epoch 1 Batch 63 Loss 0.2466\n",
      "Saving checkpoint for epoch 1 at New folder (3)\\ckpt-69\n",
      "Epoch 1 Loss 0.2466\n",
      "Time taken for 1 epoch: 899.9833707809448 secs\n",
      "\n",
      "Epoch 2 Batch 0 Loss 0.5165\n",
      "Epoch 2 Batch 1 Loss 0.3901\n",
      "Epoch 2 Batch 2 Loss 0.3499\n",
      "Epoch 2 Batch 3 Loss 0.3275\n",
      "Epoch 2 Batch 4 Loss 0.2956\n",
      "Epoch 2 Batch 5 Loss 0.3055\n",
      "Epoch 2 Batch 6 Loss 0.3194\n",
      "Epoch 2 Batch 7 Loss 0.3115\n",
      "Epoch 2 Batch 8 Loss 0.2974\n",
      "Epoch 2 Batch 9 Loss 0.2873\n",
      "Epoch 2 Batch 10 Loss 0.2841\n",
      "Epoch 2 Batch 11 Loss 0.2748\n",
      "Epoch 2 Batch 12 Loss 0.2739\n",
      "Epoch 2 Batch 13 Loss 0.2695\n",
      "Epoch 2 Batch 14 Loss 0.2567\n",
      "Epoch 2 Batch 15 Loss 0.2524\n",
      "Epoch 2 Batch 16 Loss 0.2505\n",
      "Epoch 2 Batch 17 Loss 0.2463\n",
      "Epoch 2 Batch 18 Loss 0.2430\n",
      "Epoch 2 Batch 19 Loss 0.2447\n",
      "Epoch 2 Batch 20 Loss 0.2465\n",
      "Epoch 2 Batch 21 Loss 0.2475\n",
      "Epoch 2 Batch 22 Loss 0.2446\n",
      "Epoch 2 Batch 23 Loss 0.2517\n",
      "Epoch 2 Batch 24 Loss 0.2512\n",
      "Epoch 2 Batch 25 Loss 0.2509\n",
      "Epoch 2 Batch 26 Loss 0.2498\n",
      "Epoch 2 Batch 27 Loss 0.2513\n",
      "Epoch 2 Batch 28 Loss 0.2586\n",
      "Epoch 2 Batch 29 Loss 0.2576\n",
      "Epoch 2 Batch 30 Loss 0.2588\n",
      "Epoch 2 Batch 31 Loss 0.2564\n",
      "Epoch 2 Batch 32 Loss 0.2547\n",
      "Epoch 2 Batch 33 Loss 0.2520\n",
      "Epoch 2 Batch 34 Loss 0.2537\n",
      "Epoch 2 Batch 35 Loss 0.2550\n",
      "Epoch 2 Batch 36 Loss 0.2583\n",
      "Epoch 2 Batch 37 Loss 0.2557\n",
      "Epoch 2 Batch 38 Loss 0.2556\n",
      "Epoch 2 Batch 39 Loss 0.2527\n",
      "Epoch 2 Batch 40 Loss 0.2547\n",
      "Epoch 2 Batch 41 Loss 0.2508\n",
      "Epoch 2 Batch 42 Loss 0.2498\n",
      "Epoch 2 Batch 43 Loss 0.2506\n",
      "Epoch 2 Batch 44 Loss 0.2499\n",
      "Epoch 2 Batch 45 Loss 0.2499\n",
      "Epoch 2 Batch 46 Loss 0.2499\n",
      "Epoch 2 Batch 47 Loss 0.2531\n",
      "Epoch 2 Batch 48 Loss 0.2558\n",
      "Epoch 2 Batch 49 Loss 0.2543\n",
      "Epoch 2 Batch 50 Loss 0.2564\n",
      "Epoch 2 Batch 51 Loss 0.2557\n",
      "Epoch 2 Batch 52 Loss 0.2544\n",
      "Epoch 2 Batch 53 Loss 0.2592\n",
      "Epoch 2 Batch 54 Loss 0.2594\n",
      "Epoch 2 Batch 55 Loss 0.2596\n",
      "Epoch 2 Batch 56 Loss 0.2596\n",
      "Epoch 2 Batch 57 Loss 0.2647\n",
      "Epoch 2 Batch 58 Loss 0.2644\n",
      "Epoch 2 Batch 59 Loss 0.2659\n",
      "Epoch 2 Batch 60 Loss 0.2667\n",
      "Epoch 2 Batch 61 Loss 0.2646\n",
      "Epoch 2 Batch 62 Loss 0.2643\n",
      "Epoch 2 Batch 63 Loss 0.2625\n",
      "Saving checkpoint for epoch 2 at New folder (3)\\ckpt-70\n",
      "Epoch 2 Loss 0.2625\n",
      "Time taken for 1 epoch: 724.5639581680298 secs\n",
      "\n",
      "Epoch 3 Batch 0 Loss 0.1497\n",
      "Epoch 3 Batch 1 Loss 0.1551\n",
      "Epoch 3 Batch 2 Loss 0.2138\n",
      "Epoch 3 Batch 3 Loss 0.2935\n",
      "Epoch 3 Batch 4 Loss 0.2612\n",
      "Epoch 3 Batch 5 Loss 0.2607\n",
      "Epoch 3 Batch 6 Loss 0.2537\n",
      "Epoch 3 Batch 7 Loss 0.2598\n",
      "Epoch 3 Batch 8 Loss 0.2709\n",
      "Epoch 3 Batch 9 Loss 0.2921\n",
      "Epoch 3 Batch 10 Loss 0.3136\n",
      "Epoch 3 Batch 11 Loss 0.3254\n",
      "Epoch 3 Batch 12 Loss 0.3155\n",
      "Epoch 3 Batch 13 Loss 0.3064\n",
      "Epoch 3 Batch 14 Loss 0.2969\n",
      "Epoch 3 Batch 15 Loss 0.3004\n",
      "Epoch 3 Batch 16 Loss 0.3005\n",
      "Epoch 3 Batch 17 Loss 0.3028\n",
      "Epoch 3 Batch 18 Loss 0.3021\n",
      "Epoch 3 Batch 19 Loss 0.3010\n",
      "Epoch 3 Batch 20 Loss 0.3135\n",
      "Epoch 3 Batch 21 Loss 0.3116\n",
      "Epoch 3 Batch 22 Loss 0.3188\n",
      "Epoch 3 Batch 23 Loss 0.3150\n",
      "Epoch 3 Batch 24 Loss 0.3120\n",
      "Epoch 3 Batch 25 Loss 0.3113\n",
      "Epoch 3 Batch 26 Loss 0.3125\n",
      "Epoch 3 Batch 27 Loss 0.3131\n",
      "Epoch 3 Batch 28 Loss 0.3100\n",
      "Epoch 3 Batch 29 Loss 0.3072\n",
      "Epoch 3 Batch 30 Loss 0.3043\n",
      "Epoch 3 Batch 31 Loss 0.3102\n",
      "Epoch 3 Batch 32 Loss 0.3131\n",
      "Epoch 3 Batch 33 Loss 0.3115\n",
      "Epoch 3 Batch 34 Loss 0.3126\n",
      "Epoch 3 Batch 35 Loss 0.3124\n",
      "Epoch 3 Batch 36 Loss 0.3141\n",
      "Epoch 3 Batch 37 Loss 0.3083\n",
      "Epoch 3 Batch 38 Loss 0.3088\n",
      "Epoch 3 Batch 39 Loss 0.3079\n",
      "Epoch 3 Batch 40 Loss 0.3101\n",
      "Epoch 3 Batch 41 Loss 0.3084\n",
      "Epoch 3 Batch 42 Loss 0.3091\n",
      "Epoch 3 Batch 43 Loss 0.3122\n",
      "Epoch 3 Batch 44 Loss 0.3109\n",
      "Epoch 3 Batch 45 Loss 0.3080\n",
      "Epoch 3 Batch 46 Loss 0.3059\n",
      "Epoch 3 Batch 47 Loss 0.3050\n",
      "Epoch 3 Batch 48 Loss 0.3045\n",
      "Epoch 3 Batch 49 Loss 0.3028\n",
      "Epoch 3 Batch 50 Loss 0.2990\n",
      "Epoch 3 Batch 51 Loss 0.2966\n",
      "Epoch 3 Batch 52 Loss 0.2938\n",
      "Epoch 3 Batch 53 Loss 0.2930\n",
      "Epoch 3 Batch 54 Loss 0.2919\n",
      "Epoch 3 Batch 55 Loss 0.2892\n",
      "Epoch 3 Batch 56 Loss 0.2865\n",
      "Epoch 3 Batch 57 Loss 0.2839\n",
      "Epoch 3 Batch 58 Loss 0.2804\n",
      "Epoch 3 Batch 59 Loss 0.2773\n",
      "Epoch 3 Batch 60 Loss 0.2797\n",
      "Epoch 3 Batch 61 Loss 0.2781\n",
      "Epoch 3 Batch 62 Loss 0.2762\n",
      "Epoch 3 Batch 63 Loss 0.2755\n",
      "Saving checkpoint for epoch 3 at New folder (3)\\ckpt-71\n",
      "Epoch 3 Loss 0.2755\n",
      "Time taken for 1 epoch: 624.2040042877197 secs\n",
      "\n",
      "Epoch 4 Batch 0 Loss 0.3670\n",
      "Epoch 4 Batch 1 Loss 0.3242\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-154-a28697175ec8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtar\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[1;31m# 55k samples\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "  \n",
    "    for (batch, (inp, tar)) in enumerate(dataset):\n",
    "        train_step(inp, tar)\n",
    "    \n",
    "        # 55k samples\n",
    "        # we display 3 batch results -- 0th, middle and last one (approx)\n",
    "        # 55k / 64 ~ 858; 858 / 2 = 429\n",
    "        #if batch % 50 == 0:\n",
    "        print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch + 1, batch, train_loss.result()))\n",
    "      \n",
    "    #if (epoch + 1) % 2 == 0:\n",
    "    ckpt_save_path = ckpt_manager.save()\n",
    "    print ('Saving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n",
    "    \n",
    "    print ('Epoch {} Loss {:.4f}'.format(epoch + 1, train_loss.result()))\n",
    "\n",
    "    print ('Time taken for 1 epoch: {} secs\\n'.format(time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Path to the data directory\n",
    "data_dir = Path(\"E:\\Local Disk F_122120182042\\Python\\\\New folder (2)\")\n",
    "#Get list of all the images\n",
    "images1 = list(data_dir.glob(\"*.png\"))\n",
    "# Store all the characters in a set\n",
    "characters1 = set()\n",
    "\n",
    "# A list to store the length of each captcha\n",
    "captcha_length1 = []\n",
    "\n",
    "# Store image-label info\n",
    "dataset1 = []\n",
    "\n",
    "# Iterate over the dataset and store the\n",
    "# information needed\n",
    "for img_path in images1:\n",
    "    # 1. Get the label associated with each image\n",
    "    label = img_path.name.split(\".png\")[0]\n",
    "    # 2. Store the length of this cpatcha\n",
    "    captcha_length1.append(len(label))\n",
    "    # 3. Store the image-label pair info\n",
    "    dataset1.append((str(img_path), label))\n",
    "    \n",
    "    # 4. Store the characters present\n",
    "    for ch in label:\n",
    "        characters1.add(ch)\n",
    "\n",
    "# Sort the characters        \n",
    "characters1 = sorted(characters1)\n",
    "\n",
    "# Convert the dataset info into a dataframe\n",
    "dataset1 = pd.DataFrame(dataset1, columns=[\"img_path\", \"label\"], index=None)\n",
    "\n",
    "# Shuffle the dataset\n",
    "dataset1 = dataset1.sample(frac=1.).reset_index(drop=True)\n",
    "\n",
    "\n",
    "#print(\"Number of unqiue charcaters in the whole dataset: \", len(characters))\n",
    "#print(\"Maximum length of any captcha: \", max(Counter(captcha_length).keys()))\n",
    "#print(\"Characters present: \", characters)\n",
    "##dataset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images:  (3, 50, 200)\n",
      "Number of training labels:  (3,)\n"
     ]
    }
   ],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "#training_data, validation_data = train_test_split(dataset, test_size=0.1, random_state=seed)\n",
    "\n",
    "training_data1 = dataset1\n",
    "#validation_data = validation_data.reset_index(drop=True)\n",
    "\n",
    "#print(\"Number of training samples: \", len(training_data))\n",
    "#print(\"Number of validation samples: \", len(validation_data))\n",
    "\n",
    "\n",
    "\n",
    "# Map text to numeric labels \n",
    "char_to_labels1 = {char:idx for idx, char in enumerate(characters1)}\n",
    "\n",
    "# Map numeric labels to text\n",
    "labels_to_char1 = {val:key for key, val in char_to_labels1.items()}\n",
    "\n",
    "\n",
    "\n",
    "# Sanity check for corrupted images\n",
    "def is_valid_captcha(captcha):\n",
    "    for ch in captcha:\n",
    "        if not ch in characters1:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "\n",
    "\n",
    "# Store arrays in memory as it's not a muvh big dataset\n",
    "def generate_arrays(df, resize=True, img_height=50, img_width=200):\n",
    "    \"\"\"Generates image array and labels array from a dataframe.\n",
    "    \n",
    "    Args:\n",
    "        df: dataframe from which we want to read the data\n",
    "        resize (bool)    : whether to resize images or not\n",
    "        img_weidth (int): width of the resized images\n",
    "        img_height (int): height of the resized images\n",
    "        \n",
    "    Returns:\n",
    "        images (ndarray): grayscale images\n",
    "        labels (ndarray): corresponding encoded labels\n",
    "    \"\"\"\n",
    "    \n",
    "    num_items = len(df)\n",
    "    images= np.zeros((num_items, img_height, img_width), dtype=np.float32)\n",
    "    labels = [0]*num_items\n",
    "    \n",
    "    for i in range(num_items):\n",
    "        img = cv2.imread(df[\"img_path\"][i])\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        if resize: \n",
    "            img = cv2.resize(img, (img_width, img_height))\n",
    "        \n",
    "        img = (img/255.).astype(np.float32)\n",
    "        label = df[\"label\"][i]\n",
    "        \n",
    "        # Add only if it is a valid captcha\n",
    "        if is_valid_captcha(label):\n",
    "            images[i, :, :] = img\n",
    "            labels[i] = label\n",
    "    \n",
    "    return images, np.array(labels)\n",
    "\n",
    "\n",
    "\n",
    "# Build training data\n",
    "training_data1, training_labels1 = generate_arrays(df=training_data1)\n",
    "print(\"Number of training images: \", training_data1.shape)\n",
    "print(\"Number of training labels: \", training_labels1.shape)\n",
    "\n",
    "\n",
    "# Build validation data\n",
    "#validation_data, validation_labels = generate_arrays(df=validation_data)\n",
    "#print(\"Number of validation images: \", validation_data.shape)\n",
    "#print(\"Number of validation labels: \", validation_labels.shape)\n",
    "row_to_be_added = np.array(np.ones(200)) \n",
    "train_data=np.zeros(shape=(3,200,200), dtype=np.float32)\n",
    "for j in range(3):\n",
    "    result=training_data1[j]\n",
    "    for i in range(150):\n",
    "# Adding row to numpy array \n",
    "        result = np.vstack ((result, row_to_be_added) ) \n",
    "    train_data[j]=result\n",
    "    \n",
    "#valid_data=np.zeros(shape=(103,200,200), dtype=np.float32)\n",
    "#for j in range(103):\n",
    "  #  result=validation_data[j]\n",
    "   # for i in range(150):\n",
    "# Adding row to numpy array \n",
    "    #    result = np.vstack ((result, row_to_be_added) ) \n",
    "   # valid_data[j]=result#\n",
    "# Function to convert   \n",
    "def listToString(s):  \n",
    "    \n",
    "    # initialize an empty string \n",
    "    str1 = \"<go> \"  \n",
    "    \n",
    "    # traverse in the string   \n",
    "    for ele in s:  \n",
    "        str1 += ele   \n",
    "        str1 += \" \"\n",
    "    # return string   \n",
    "    return str1+ '<stop>'  \n",
    "# Python3 program to Split string into characters \n",
    "def split(word): \n",
    "    return [char for char in word] \n",
    "train_labl_result=[0]*3\n",
    "#valed_labl_result=[0]*103\n",
    "for i in range(3):\n",
    "    train_labl_result[i]=listToString(split(training_labels1[i]))\n",
    "#for i in range(103):\n",
    " #   valed_labl_result[i]=listToString(split(validation_labels[i]))\n",
    "# Driver code \n",
    "#word = validation_labels[0]\n",
    "#print(split(word)) \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in range(1021):\n",
    "  #  h=train_data[i].T\n",
    "#h,train_data[0]\n",
    "h=np.zeros(shape=(3,200,200), dtype=np.float32)\n",
    "for j in range(3):\n",
    "    h[j]=train_data[j].T\n",
    "   # for i in range(150):\n",
    "# Adding row to numpy array \n",
    "    #result = np.vstack ((result, row_to_be_added) ) \n",
    "   # train_data[j]=result\n",
    "hg=np.concatenate((h, train_data), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 400, 200)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(input_img):\n",
    "    #input_document = document_tokenizer.texts_to_sequences([input_document])\n",
    "    #input_document = tf.keras.preprocessing.sequence.pad_sequences(input_document, maxlen=encoder_maxlen, padding='post', truncating='post')\n",
    "    encoder_input = input_img\n",
    "    encoder_input = tf.expand_dims(encoder_input, 0)\n",
    "\n",
    "    decoder_input = tf.cast([2.], dtype=tf.float32)\n",
    "    output = tf.expand_dims(decoder_input, 0)\n",
    "    \n",
    "    for i in range(7):\n",
    "        enc_padding_mask, combined_mask, dec_padding_mask = create_masks(encoder_input, output)\n",
    "\n",
    "        predictions,_ = transformer66(\n",
    "            encoder_input, \n",
    "            output,\n",
    "            False,\n",
    "            None,\n",
    "            combined_mask,\n",
    "            None\n",
    "        )\n",
    "\n",
    "        predictions = predictions[: ,-1:, :]\n",
    "        predicted_id = tf.cast(tf.argmax(predictions, axis=-1), tf.float32)\n",
    "\n",
    "        if predicted_id == [3.]:\n",
    "            return tf.squeeze(output, axis=0)\n",
    "\n",
    "        output = tf.concat([output, predicted_id], axis=-1)\n",
    "\n",
    "    return tf.squeeze(output, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize(input_img):\n",
    "    # not considering attention weights for now, can be used to plot attention heatmaps in the future\n",
    "    summarized = evaluate(input_img)\n",
    "    summarized = np.expand_dims(summarized[1:], 0)  # not printing <go> token\n",
    "    return document_tokenizer.sequences_to_texts(summarized)[0] # since there is just one translated document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "w 2 n 7 w\n",
      "1\n",
      "3 3 3 m x\n",
      "4\n",
      "4 4 c n 7\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    print(training_labels1[i]),print(summarize(hg[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'e p p g 8'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(hg[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7840beec88>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(hg[16])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"transformer_12\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_12 (Encoder)         multiple                  966600    \n",
      "_________________________________________________________________\n",
      "decoder_11 (Decoder)         multiple                  1455200   \n",
      "_________________________________________________________________\n",
      "dense_507 (Dense)            multiple                  5025      \n",
      "=================================================================\n",
      "Total params: 2,426,825\n",
      "Trainable params: 2,426,825\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "transformer66.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<unk>': 1,\n",
       " 'go': 2,\n",
       " 'stop': 3,\n",
       " 'n': 4,\n",
       " '4': 5,\n",
       " 'm': 6,\n",
       " '5': 7,\n",
       " '3': 8,\n",
       " 'f': 9,\n",
       " 'g': 10,\n",
       " '8': 11,\n",
       " 'x': 12,\n",
       " '2': 13,\n",
       " 'c': 14,\n",
       " 'd': 15,\n",
       " '6': 16,\n",
       " '7': 17,\n",
       " 'p': 18,\n",
       " 'b': 19,\n",
       " 'e': 20,\n",
       " 'w': 21,\n",
       " 'y': 22}"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 400, 200), dtype=float32, numpy=\n",
       "array([[[0.7529412, 0.7529412, 0.7529412, ..., 1.       , 1.       ,\n",
       "         1.       ],\n",
       "        [0.7529412, 0.7529412, 0.7529412, ..., 1.       , 1.       ,\n",
       "         1.       ],\n",
       "        [0.7529412, 0.7529412, 0.7529412, ..., 1.       , 1.       ,\n",
       "         1.       ],\n",
       "        ...,\n",
       "        [1.       , 1.       , 1.       , ..., 1.       , 1.       ,\n",
       "         1.       ],\n",
       "        [1.       , 1.       , 1.       , ..., 1.       , 1.       ,\n",
       "         1.       ],\n",
       "        [1.       , 1.       , 1.       , ..., 1.       , 1.       ,\n",
       "         1.       ]]], dtype=float32)>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input = hg[0]\n",
    "output = tf.expand_dims(hg[0], 0)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
